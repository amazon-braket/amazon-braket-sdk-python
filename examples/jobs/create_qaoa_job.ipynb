{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f864c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from braket.aws import AwsQuantumJob, AwsSession\n",
    "\n",
    "from braket.jobs.config import (\n",
    "    DataSource,\n",
    "    InputDataConfig,\n",
    "    InstanceConfig,\n",
    "    OutputDataConfig,\n",
    "    S3DataSource,\n",
    "    StoppingCondition\n",
    ")\n",
    "\n",
    "from braket.jobs.metrics_data.definitions import MetricType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "949e1ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify to whatever alias. Only used in naming the job\n",
    "alias = 'your-alias'\n",
    "\n",
    "# Pick 'autograd', 'tf', or 'torch'. Autograd is base PennyLane\n",
    "interface = \"autograd\"\n",
    "# SV1, DM1, TN1, or Rigetti. IonQ may take too long\n",
    "device_arn = \"arn:aws:braket:::device/quantum-simulator/amazon/sv1\"\n",
    "# Specify previous job arn to copy checkpoints from, or None to not copy checkpoints\n",
    "copy_checkpoints_from_job = None\n",
    "# copy_checkpoints_from_job = \"arn:aws:braket:us-west-2:<your-account-id>:job/<job-name>\"\n",
    "\n",
    "hyperparameters = {\n",
    "    # We generate a random graph with num_nodes nodes and num_edges edges to run Max Cut on.\n",
    "    # Number of tasks per iteration = 2 * (num_nodes + num_edges) * p + 1\n",
    "    # num_nodes is the number of qubits, so each circuit will have runtime exponential in num_nodes\n",
    "    \"num_nodes\": \"4\",\n",
    "    \"num_edges\": \"5\",\n",
    "    \"p\": \"2\",\n",
    "    \"seed\": \"1967\",\n",
    "    # Maximum number of simultaneous tasks allowed\n",
    "    \"max_parallel\": \"30\",\n",
    "    # Number of total optimization iterations, including those from previous checkpoint (if any)\n",
    "    \"num_iterations\": \"3\",\n",
    "    \"interface\": interface,\n",
    "}\n",
    "if copy_checkpoints_from_job:\n",
    "    hyperparameters['copy_checkpoints_from_job'] = copy_checkpoints_from_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497313b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Job data will be sent to this bucket by default,\n",
    "# unless output_data_config is specified in the next cell.\n",
    "aws_session = AwsSession()\n",
    "bucket = aws_session.default_bucket()\n",
    "print(bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84e4ad50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other settings to test\n",
    "# Uncomment or edit to vary\n",
    "kwargs = {\n",
    "#     \"stopping_condition\": StoppingCondition(maxRuntimeInSeconds=5 * 24 * 60 * 60),\n",
    "#     \"output_data_config\": OutputDataConfig(s3Path=\"s3://Your-Bucket-Name\"),\n",
    "#     \"instance_config\": InstanceConfig(instanceType=\"ml.m5.large\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5370e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to send the Max Cut graph as input data.\n",
    "# The num_nodes and num_edges hyperparameters will be ignored inside the job,\n",
    "# since the graph is generated here instead\n",
    "\n",
    "# import tempfile\n",
    "# import networkx as nx\n",
    "# import numpy as np\n",
    "\n",
    "# def setup_input_stream():\n",
    "#     graph = nx.gnm_random_graph(\n",
    "#         int(hyperparameters[\"num_nodes\"]),\n",
    "#         int(hyperparameters[\"num_edges\"]),\n",
    "#         seed=int(hyperparameters[\"seed\"])\n",
    "#     )\n",
    "#     with tempfile.NamedTemporaryFile() as temp_graph_file:\n",
    "#         # define the uri for our input data stream\n",
    "#         stream_s3_uri = aws_session.construct_s3_uri(\n",
    "#             aws_session.default_bucket(),\n",
    "#             \"input-graph\",\n",
    "#         )\n",
    "#         # upload something to our input stream location\n",
    "#         nx.write_adjlist(graph, temp_graph_file.name)\n",
    "#         aws_session.upload_to_s3(temp_graph_file.name, f\"{stream_s3_uri}/input-data.adjlist\")\n",
    "#     return stream_s3_uri\n",
    "\n",
    "# kwargs[\"input_data_config\"] = [InputDataConfig(\n",
    "#     channelName=\"input-graph\",\n",
    "#     dataSource=DataSource(\n",
    "#         s3DataSource=S3DataSource(\n",
    "#             s3Uri=setup_input_stream(),\n",
    "#         ),\n",
    "#     ),\n",
    "# )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3731ba4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "region = aws_session.region\n",
    "\n",
    "# Choose the container based on which one we need.\n",
    "if interface == 'autograd':\n",
    "    # Base container does not have PennyLane, so following doesn't work\n",
    "    # image_uri = f\"292282985366.dkr.ecr.{region}.amazonaws.com/base-jobs:1.0-cpu-py37-ubuntu18.04\"\n",
    "    image_uri=f\"292282985366.dkr.ecr.{region}.amazonaws.com/tensorflow-jobs:2.4.1-cpu-py37-ubuntu18.04\"\n",
    "elif interface == 'tf':\n",
    "    image_uri=f\"292282985366.dkr.ecr.{region}.amazonaws.com/tensorflow-jobs:2.4.1-cpu-py37-ubuntu18.04\"\n",
    "elif interface == 'torch':\n",
    "    image_uri=f\"292282985366.dkr.ecr.{region}.amazonaws.com/pytorch-jobs:1.8.1-cpu-py37-ubuntu18.04\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b814b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:braket:us-west-2:491006770141:job/JobTest-autograd-licedric-1631749213\n",
      "INITIALIZED\n",
      "RUNNING\n",
      "RUNNING\n",
      "RUNNING\n",
      "RUNNING\n",
      "RUNNING\n",
      "RUNNING\n",
      "RUNNING\n",
      "RUNNING\n",
      "RUNNING\n",
      "RUNNING\n",
      "RUNNING\n",
      "RUNNING\n",
      "RUNNING\n",
      "RUNNING\n",
      "RUNNING\n",
      "RUNNING\n",
      "RUNNING\n",
      "RUNNING\n",
      "RUNNING\n",
      "RUNNING\n",
      "RUNNING\n",
      "RUNNING\n",
      "RUNNING\n",
      "RUNNING\n",
      "RUNNING\n",
      "RUNNING\n",
      "RUNNING\n",
      "RUNNING\n",
      "COMPLETED\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "job = AwsQuantumJob.create(\n",
    "    # Note 63-character limit in job name\n",
    "    job_name=\"JobTest-\"+interface+\"-\"+alias+\"-\"+str(int(time.time())),\n",
    "    image_uri=image_uri,\n",
    "    # Relative to the current location\n",
    "    entry_point=\"source_dir.qaoa_entry_point:start_here\",\n",
    "    device_arn=device_arn,\n",
    "    source_module=\"source_dir\",\n",
    "    copy_checkpoints_from_job=copy_checkpoints_from_job,\n",
    "    # general parameters\n",
    "    hyperparameters=hyperparameters,\n",
    "    # Uncomment the following to make this create() call wait for job completion\n",
    "    # If so, the CloudWatch logs will be printed to the console as well\n",
    "    # wait_until_complete=True,\n",
    "    **kwargs\n",
    ")\n",
    "\n",
    "print(job.arn)\n",
    "# Poll the job state asynchronously\n",
    "while job.state() not in AwsQuantumJob.TERMINAL_STATES:\n",
    "    print(job.state())\n",
    "    time.sleep(10)\n",
    "\n",
    "end_time = time.time()\n",
    "print(job.state())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1fc8996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "307.2561218738556\n"
     ]
    }
   ],
   "source": [
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a173656c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(job.metadata())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0fa9e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'params': [[0.04640119576374078, -0.0006687931947795414], [-0.052651388020882574, -0.03491989905871668]], 'cost': -2.534}\n"
     ]
    }
   ],
   "source": [
    "print(job.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d52f9012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'timestamp': [1631749511.6675212, 1631749508.9265392, 1631749497.1312518, 1631749487.3299847], 'Cost': [-2.534, -2.517, -2.508, -2.495], 'iteration_number': [3.0, 2.0, 1.0, 0.0]}\n"
     ]
    }
   ],
   "source": [
    "# May need to wait a bit before metrics show up\n",
    "# If metrics aren't there, run a bit later\n",
    "time.sleep(120)\n",
    "print(job.metrics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee3646f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to print logs from CloudWatch\n",
    "job.logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de3ef9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloads results to current directory\n",
    "job.download_result()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_braket",
   "language": "python",
   "name": "conda_braket"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
